{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"MT5 Small Model.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"e660b241"},"source":["# MT5 Small Model\n","\n","In this notebook, we will be fine tuning the MT5 Sequence-to-Sequence Transformer model to take a Natural Language structured card specification to Java code."],"id":"e660b241"},{"cell_type":"markdown","metadata":{"id":"14974d65"},"source":["### Check for Cuda Compatibility."],"id":"14974d65"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"833e8cea","executionInfo":{"status":"ok","timestamp":1620765891455,"user_tz":-240,"elapsed":1140,"user":{"displayName":"M Sob","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKu2CnMkaZoef3LWM8Rfdx4c3r6oNQURp9FTC7=s64","userId":"10920003909433523408"}},"outputId":"069dcf11-7af9-4c9c-b9ca-8af80d7af4e7"},"source":["import torch\n","import torch.nn as nn\n","torch.cuda.is_available()"],"id":"833e8cea","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cxkouxtHOmFr","executionInfo":{"status":"ok","timestamp":1620765891457,"user_tz":-240,"elapsed":1129,"user":{"displayName":"M Sob","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKu2CnMkaZoef3LWM8Rfdx4c3r6oNQURp9FTC7=s64","userId":"10920003909433523408"}},"outputId":"7188d504-2d4c-4aa5-f813-6631e00f67f9"},"source":["using_google_drive = True\n","\n","if using_google_drive:\n","    from google.colab import drive\n","    drive.mount('/content/gdrive')\n","    mahmoud_path = '/content/gdrive/MyDrive/Final Project/'\n","    tommy_path = '/content/gdrive/MyDrive/Colab Notebooks/Final Project/'\n","    path = mahmoud_path\n","    PATH = path"],"id":"cxkouxtHOmFr","execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4288262e"},"source":["%%bash\n","pip -q install transformers\n","pip -q install tqdm\n","pip -q install sentencepiece "],"id":"4288262e","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a49a3c2f"},"source":["# Tokenizer for the MT5 Model"],"id":"a49a3c2f"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"614590d1","executionInfo":{"status":"ok","timestamp":1620765903903,"user_tz":-240,"elapsed":13560,"user":{"displayName":"M Sob","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKu2CnMkaZoef3LWM8Rfdx4c3r6oNQURp9FTC7=s64","userId":"10920003909433523408"}},"outputId":"08595be0-e4d3-4b45-aa18-092d195ee6f8"},"source":["# Tokenizers\n","import transformers\n","pretrained_model_name = 'google/mt5-small'\n","\n","tokenizer = transformers.AutoTokenizer.from_pretrained(pretrained_model_name)\n","context_ids = tokenizer.encode(\"When this creature enters the battle field, target creature loses 2 life.\")\n","print(tokenizer.convert_ids_to_tokens(context_ids))"],"id":"614590d1","execution_count":null,"outputs":[{"output_type":"stream","text":["['▁', 'When', '▁this', '▁c', 'reature', '▁enter', 's', '▁the', '▁battle', '▁field', ',', '▁target', '▁c', 'reature', '▁los', 'es', '▁2', '▁life', '.', '</s>']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"58eb6160","executionInfo":{"status":"ok","timestamp":1620765903905,"user_tz":-240,"elapsed":13555,"user":{"displayName":"M Sob","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKu2CnMkaZoef3LWM8Rfdx4c3r6oNQURp9FTC7=s64","userId":"10920003909433523408"}},"outputId":"d8670f2c-5675-4348-dbca-920418c0018d"},"source":["ctx_list = [\n","    \"You can protect yourself by wearing an N95 mask.\", \n","    \"wearing an N95 mask\"\n","]\n","\n","tokenizer_output = tokenizer.batch_encode_plus(\n","    ctx_list,\n","    max_length = 12,\n","    truncation=True,\n","    padding='longest',\n","    return_attention_mask=True,\n","    return_tensors='pt'\n",")\n","\n","for key, value in tokenizer_output.items():\n","    print(key)\n","    print(value)\n","    print('-------------------------')"],"id":"58eb6160","execution_count":null,"outputs":[{"output_type":"stream","text":["input_ids\n","tensor([[ 1662,   738, 36478, 23191,   455, 25972,   347,   461,   441,  4910,\n","          6408,     1],\n","        [25972,   347,   461,   441,  4910,  6408,     1,     0,     0,     0,\n","             0,     0]])\n","-------------------------\n","attention_mask\n","tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])\n","-------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bbf8d9b7"},"source":["# Dataset Collection and Processing\n","\n","Load the dataset. The framework for making changes to individual points in the dataset is set in the `preprocess_datapoint` method, which at the moment does nothing to our dataset."],"id":"bbf8d9b7"},{"cell_type":"code","metadata":{"id":"bfd61e97"},"source":["with open(PATH + 'datasets_uwu/train_magic.in') as f:\n","    train_x = f.readlines()\n","with open(PATH + 'datasets_uwu/train_magic.out') as f:\n","    train_y = f.readlines()\n","with open(PATH + 'datasets_uwu/test_magic.in') as f:\n","    test_x = f.readlines()\n","with open(PATH + 'datasets_uwu/test_magic.out') as f:\n","    test_y = f.readlines()\n","# Structure the dataset somewhat similarly to the dataset objects\n","training_dataset = [{ 'card': x, 'code': y } for x, y in zip(train_x, train_y)]\n","testing_dataset  = [{ 'card': x, 'code': y } for x, y in zip(test_x,  test_y )]\n","\n","dataset = {\n","    \"train\": training_dataset,\n","    \"test\":  testing_dataset\n","}"],"id":"bfd61e97","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c1b8e1b5","executionInfo":{"status":"ok","timestamp":1620765905775,"user_tz":-240,"elapsed":15411,"user":{"displayName":"M Sob","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKu2CnMkaZoef3LWM8Rfdx4c3r6oNQURp9FTC7=s64","userId":"10920003909433523408"}},"outputId":"93e041ee-f614-429d-a66c-c4f8ca814b2b"},"source":["import json\n","import random\n","from multiprocessing import Pool\n","from tqdm import tqdm, trange\n","\n","\n","def preproc_init(tokenizer_for_model):\n","    \"\"\" \n","    Use this to assign global variables within a new thread \n","    \n","    Parameters\n","    ----------\n","    tokenizer_for_mode: fn\n","        The tokenizer for the pretrained transformer model\n","    \"\"\"\n","    global tokenizer\n","    tokenizer = tokenizer_for_model\n","\n","def preprocess_datapoint (datapoint):\n","    \"\"\"\n","    Is effectively an identity function, but is here if we do preprocessing later\n","    \n","    This method will preprocess a single datapoint loaded above. This can involve\n","    replacing characters, removing parts of the input or output, etc. The current\n","    implementation applies no change to the dict. It can return None if we want to \n","    remove this datapoint as well.\n","\n","    Parameters\n","    ----------\n","    datapoint: dict\n","        The dict containing the initial value of each data in the dataset.\n","        Each datapoint has the following two datapoints\n","        \"card\": the string for the card description and meta data\n","        \"code\": the string for the card implementation in Java\n","    \n","    Returns\n","    -------\n","    dict\n","        A new representation for this individual datapoint.\n","    \"\"\"\n","    \n","    # We have access to global vars defined in preproc_init\n","    return datapoint\n","    \n","    \n","def preprocess_dataset(dataset_list, threads, tokenizer):\n","    \"\"\"\n","    Preprocesses the entire dataset in `threads` threads\n","    \n","    This method opens `threads` new threads in\n","\n","    Parameters\n","    ----------\n","    dataset_list: dict[]\n","        A list of datapoints, where each datapoint is in the shape:\n","        \"card\": the string for the card description and meta data\n","        \"code\": the string for the card implementation in Java\n","    threads: int\n","        The number of threads to run the preprocessing on\n","    tokenizer: fn\n","        The tokenizer for the particular pretrained model\n","    \n","    Returns\n","    -------\n","    dict\n","        A new representation for every datapoint in the dataset_list\n","    \"\"\"\n","    \n","    # Open new threads and map tasks between them\n","    with Pool(threads, initializer=preprocess_datapoint, initargs=(tokenizer,)) as p:\n","        processed_dataset = list(tqdm(p.imap(preprocess_datapoint, dataset_list), total=len(dataset_list)))\n","    # Remove None values in the list\n","    processed_dataset = [x for x in processed_dataset if x]\n","    \n","    json.dump(processed_dataset, open(PATH + \"/processed_dataset.json\", 'w'))\n","    return processed_dataset\n","\n","processed_dataset = preprocess_dataset(dataset['train'], 16, tokenizer)\n"],"id":"c1b8e1b5","execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 11969/11969 [00:01<00:00, 7873.45it/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"ed0f70c1"},"source":["# Building the Model"],"id":"ed0f70c1"},{"cell_type":"code","metadata":{"id":"4a47d71c"},"source":["\n","class ModelOutputs:\n","    def __init__(self, output_logits=None, loss=None):\n","        \"\"\"\n","        An object containing the output of the CardTranslationModel\n","        \n","        Parameters\n","        ----------\n","        output_logits : torch.tensor\n","            shape (batch_size, ans_len)\n","        loss : torch.tensor\n","            shape (1) The loss of the output\n","\n","        \"\"\"\n","        self.output_logits = output_logits\n","        self.loss = loss\n","        \n","class CardTranslationModel(nn.Module):\n","\n","    def __init__(self, lm=None):\n","        \"\"\"\n","        Initializes the CardTranslationModel with the provided learning mdoel\n","\n","        Parameters\n","        ----------\n","        lm : pretrained transformer\n","            Description of arg1\n","\n","        \"\"\"\n","        super(CardTranslationModel, self).__init__()\n","        self.lm = lm\n","    \n","    def forward(self, input_ids=None, attention_mask=None, label_ids=None):\n","        \"\"\"\n","        Summary line.\n","\n","        Extended description of function.\n","\n","        Parameters\n","        ----------\n","        input_ids : torch.tensor\n","            shape (batch_size, seq_len) ids of the concatenated input tokens\n","        attention_mask : torch.tensor\n","            shape (batch_size, seq_len) concatenated attention masks\n","        label_ids: torch.tensor\n","            shape (batch_size, ans_len) the expected code output\n","\n","        Returns\n","        -------\n","        ModelOutputs\n","            Description of return value\n","\n","        \"\"\"\n","        # Feed our input ids into the pretrained transformer\n","        lm_output = self.lm(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            labels=label_ids,\n","            use_cache=False\n","        )\n","        \n","        # Compute Loss from the output of the learning model\n","        total_loss = lm_output['loss']\n","        if False and label_ids is not None:\n","            loss_fct = nn.CrossEntropyLoss()\n","            total_loss = loss_fct(label_ids, lm_output['logits'])\n","            \n","        return ModelOutputs(\n","            output_logits=lm_output['logits'],\n","            loss=total_loss)\n","        "],"id":"4a47d71c","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"af30dbc7"},"source":["from transformers import MT5ForConditionalGeneration, T5Tokenizer\n","\n","tokenizer = T5Tokenizer.from_pretrained(pretrained_model_name)\n","# Create the CardTranslationModel using the MT5 Conditional Generation model\n","lm_pretrained = MT5ForConditionalGeneration.from_pretrained(pretrained_model_name)\n","model = CardTranslationModel(lm_pretrained).cuda()"],"id":"af30dbc7","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"58c019f2"},"source":["## Up Next Training:"],"id":"58c019f2"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"93b1c1fa","executionInfo":{"status":"ok","timestamp":1620765919315,"user_tz":-240,"elapsed":28925,"user":{"displayName":"M Sob","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKu2CnMkaZoef3LWM8Rfdx4c3r6oNQURp9FTC7=s64","userId":"10920003909433523408"}},"outputId":"596e7171-5850-40e8-9b0a-bcbdce1ef8bf"},"source":["import torch\n","\n","# Hyper-parameters: you could try playing with different settings\n","num_epochs = 1\n","learning_rate = 3e-5\n","weight_decay = 1e-5\n","eps = 1e-6\n","batch_size = 2\n","warmup_rate = 0.05\n","card_max_length = 448\n","code_max_length = 448\n","\n","# Calculating the number of warmup steps\n","num_training_cases = len(processed_dataset)\n","t_total = (num_training_cases // batch_size + 1) * num_epochs\n","ext_warmup_steps = int(warmup_rate * t_total)\n","\n","# Initializing an AdamW optimizer\n","ext_optim = torch.optim.AdamW(model.parameters(), lr=learning_rate,\n","                              eps=eps, weight_decay=weight_decay)\n","\n","# Initializing the learning rate scheduler [details are in the BERT paper]\n","# ext_sche = transformers.get_linear_schedule_with_warmup(\n","#     ext_optim, num_warmup_steps=ext_warmup_steps, num_training_steps=t_total\n","# )\n","\n","print(\"***** Training Info *****\")\n","print(\"  Num examples = %d\" % t_total)\n","print(\"  Num Epochs = %d\" % num_epochs)\n","print(\"  Batch size = %d\" % batch_size)\n","print(\"  Total optimization steps = %d\" % t_total)"],"id":"93b1c1fa","execution_count":null,"outputs":[{"output_type":"stream","text":["***** Training Info *****\n","  Num examples = 5985\n","  Num Epochs = 1\n","  Batch size = 2\n","  Total optimization steps = 5985\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"be52333c"},"source":["def vectorize_batch(batch, tokenizer):\n","    \"\"\"\n","    Converts the batch of processed datapoints into separate tensors of token ids\n","    \n","    Converts the batch of processed datapoints into separate tensors of token ids\n","    hosted on the GPU. \n","    \n","    Parameters\n","    ----------\n","    batch: str[]\n","        shape (batch_size, 1)\n","    tokenizer: fn\n","        Converts the batch to a tensor of input and output ids\n","    \n","    Returns\n","    -------\n","    input_ids: torch.tensor\n","        shape (batch_size, max_input_len)\n","    input_attn_mask: torch.tensor\n","        shape (batch_size, max_input_len)\n","    label_ids: torch.tensor\n","        shape (batch_size, max_output_len)\n","    \"\"\"\n","    \n","    # Separate the batch into input and output\n","    card_batch = [card_data['card'] for card_data in batch]\n","    code_batch = [code_data['code'] for code_data in batch]\n","    \n","    # Encode the card's natural language representation\n","    card_encode = tokenizer.batch_encode_plus(\n","        card_batch,\n","        max_length = card_max_length,\n","        truncation = True,\n","        padding = 'longest',\n","        return_attention_mask = True,\n","        return_tensors = 'pt'\n","    )\n","\n","    # Encode the card's java code representation\n","    code_encode = tokenizer.batch_encode_plus(\n","        code_batch,\n","        max_length = code_max_length,\n","        truncation = True,\n","        padding = 'longest',\n","        return_attention_mask = True,\n","        return_tensors = 'pt'\n","    )\n","    \n","    # Move the training batch to GPU\n","    card_ids        = card_encode['input_ids'].cuda()\n","    card_attn_mask  = card_encode['attention_mask'].cuda()\n","    code_ids        = code_encode['input_ids'].cuda()\n","    \n","    return card_ids, card_attn_mask, code_ids\n","    \n","    "],"id":"be52333c","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"117272c8","executionInfo":{"status":"ok","timestamp":1620767502615,"user_tz":-240,"elapsed":1612213,"user":{"displayName":"M Sob","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKu2CnMkaZoef3LWM8Rfdx4c3r6oNQURp9FTC7=s64","userId":"10920003909433523408"}},"outputId":"4fbb60a9-b9e1-4896-93d4-0972b7247a6d"},"source":["import gc\n","\n","model.train()\n","max_grad_norm = 1\n","\n","training_dataset = processed_dataset[:6000]\n","num_training_cases = len(training_dataset)\n","\n","step_id = 0\n","for _ in range(num_epochs):\n","\n","    random.shuffle(training_dataset)\n","\n","    for i in range(0, num_training_cases, batch_size):\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","        batch = training_dataset[i: i + batch_size]\n","        input_ids, input_attn_mask, label_ids = vectorize_batch(batch, tokenizer)\n","\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","        model.zero_grad() # Does the same as ext_optim.zero_grad()\n","\n","        # Get the model outputs, including (start, end) logits and losses\n","        # stored as a ModelOutput object\n","        outputs = model(            \n","            input_ids=input_ids,\n","            attention_mask=input_attn_mask,\n","            label_ids=label_ids\n","        )\n","        \n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","        # Back-propagate the loss signal and clip the gradients\n","        loss = outputs.loss.mean()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","\n","        # Update neural network parameters and the learning rate\n","        ext_optim.step()\n","        # ext_sche.step() # Update learning rate for better convergence\n","\n","        if step_id % 100 == 0:\n","            print(f'At step {step_id}, the extraction loss = {loss}')\n","        \n","        step_id += 1\n","\n","        input_ids.detach()\n","        input_attn_mask.detach()\n","        label_ids.detach()\n","        outputs.loss.detach()\n","        \n","        del input_ids\n","        del input_attn_mask\n","        del label_ids\n","        del outputs\n","\n","        torch.cuda.empty_cache()\n","\n","print('Finished Training')"],"id":"117272c8","execution_count":null,"outputs":[{"output_type":"stream","text":["At step 0, the extraction loss = 25.254150390625\n","At step 100, the extraction loss = 18.907732009887695\n","At step 200, the extraction loss = 13.932411193847656\n","At step 300, the extraction loss = 9.968170166015625\n","At step 400, the extraction loss = 8.454705238342285\n","At step 500, the extraction loss = 7.8662261962890625\n","At step 600, the extraction loss = 5.209946632385254\n","At step 700, the extraction loss = 4.320058822631836\n","At step 800, the extraction loss = 3.4333794116973877\n","At step 900, the extraction loss = 2.139693021774292\n","At step 1000, the extraction loss = 2.3690686225891113\n","At step 1100, the extraction loss = 1.8576878309249878\n","At step 1200, the extraction loss = 2.5573413372039795\n","At step 1300, the extraction loss = 2.447085380554199\n","At step 1400, the extraction loss = 2.2938404083251953\n","At step 1500, the extraction loss = 1.7430731058120728\n","At step 1600, the extraction loss = 1.3317904472351074\n","At step 1700, the extraction loss = 1.7130123376846313\n","At step 1800, the extraction loss = 1.9508438110351562\n","At step 1900, the extraction loss = 1.347658395767212\n","At step 2000, the extraction loss = 1.7633243799209595\n","At step 2100, the extraction loss = 1.2397438287734985\n","At step 2200, the extraction loss = 1.509194254875183\n","At step 2300, the extraction loss = 0.9653919339179993\n","At step 2400, the extraction loss = 1.31890869140625\n","At step 2500, the extraction loss = 1.457659125328064\n","At step 2600, the extraction loss = 1.8935260772705078\n","At step 2700, the extraction loss = 1.313441276550293\n","At step 2800, the extraction loss = 0.8529648184776306\n","At step 2900, the extraction loss = 0.5246104598045349\n","Finished Training\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"41543893"},"source":["torch.save(model.state_dict(), '/content/gdrive/MyDrive/MT5_checkpoint/MT5.pt')"],"id":"41543893","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"99fbf66a","executionInfo":{"status":"ok","timestamp":1620767776944,"user_tz":-240,"elapsed":914,"user":{"displayName":"M Sob","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKu2CnMkaZoef3LWM8Rfdx4c3r6oNQURp9FTC7=s64","userId":"10920003909433523408"}},"outputId":"4fb9e55d-a095-4bd6-ba8c-fd046ce71f18"},"source":["import torch\n","import gc\n","gc.collect()\n","torch.cuda.empty_cache()\n","print(torch.cuda.memory_summary(device=None, abbreviated=False))"],"id":"99fbf66a","execution_count":null,"outputs":[{"output_type":"stream","text":["|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    4589 MB |    9100 MB |   29047 GB |   29042 GB |\n","|       from large pool |    4301 MB |    8742 MB |   26755 GB |   26751 GB |\n","|       from small pool |     288 MB |     540 MB |    2291 GB |    2291 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    4589 MB |    9100 MB |   29047 GB |   29042 GB |\n","|       from large pool |    4301 MB |    8742 MB |   26755 GB |   26751 GB |\n","|       from small pool |     288 MB |     540 MB |    2291 GB |    2291 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |    5822 MB |    9460 MB |    9195 GB |    9189 GB |\n","|       from large pool |    5438 MB |    8862 MB |    8885 GB |    8880 GB |\n","|       from small pool |     384 MB |     600 MB |     309 GB |     309 GB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    1232 MB |    2342 MB |   14893 GB |   14892 GB |\n","|       from large pool |    1136 MB |    2240 MB |   12325 GB |   12323 GB |\n","|       from small pool |      95 MB |     109 MB |    2568 GB |    2568 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     761    |    1306    |   10169 K  |   10168 K  |\n","|       from large pool |     200    |     580    |    3386 K  |    3385 K  |\n","|       from small pool |     561    |    1025    |    6783 K  |    6783 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     761    |    1306    |   10169 K  |   10168 K  |\n","|       from large pool |     200    |     580    |    3386 K  |    3385 K  |\n","|       from small pool |     561    |    1025    |    6783 K  |    6783 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |     221    |     336    |  171984    |  171763    |\n","|       from large pool |      29    |      53    |   13516    |   13487    |\n","|       from small pool |     192    |     300    |  158468    |  158276    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |     247    |     363    |    4206 K  |    4205 K  |\n","|       from large pool |      22    |      28    |    1962 K  |    1962 K  |\n","|       from small pool |     225    |     344    |    2243 K  |    2243 K  |\n","|===========================================================================|\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8113c214","colab":{"base_uri":"https://localhost:8080/","height":320},"executionInfo":{"status":"error","timestamp":1620765756198,"user_tz":-240,"elapsed":1278,"user":{"displayName":"M Sob","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKu2CnMkaZoef3LWM8Rfdx4c3r6oNQURp9FTC7=s64","userId":"10920003909433523408"}},"outputId":"4e33d319-fcff-4738-e2ec-e873ddc0fc4a"},"source":["\n","model.load_state_dict(torch.load(PATH + '/checkpoint'))"],"id":"8113c214","execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-b75ea7978db6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/checkpoint'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;31m# reset back to the original position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0morig_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m_is_torchscript_zip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m                     warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:145] . PytorchStreamReader failed reading zip archive: failed finding central directory"]}]},{"cell_type":"code","metadata":{"id":"64ee9f0e"},"source":["#---Sandbox code---#\n","\n","import random\n","batch_size = 2\n","i = random.randint(0, num_training_cases-1)\n","\n","datapoint = training_dataset[i:i + batch_size]\n","#print(datapoint)\n","card_ids, card_attn_masks, code_ids = vectorize_batch(datapoint, tokenizer)\n","\n","output = model(card_ids, card_attn_masks, code_ids)\n","\n"],"id":"64ee9f0e","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"59e34677","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620771782617,"user_tz":-240,"elapsed":1333,"user":{"displayName":"M Sob","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKu2CnMkaZoef3LWM8Rfdx4c3r6oNQURp9FTC7=s64","userId":"10920003909433523408"}},"outputId":"70c11516-45a1-4af2-b0e5-298e370d64dc"},"source":["#---Sandbox code---#\n","\n","#print(output.output_logits)\n","softmax = torch.nn.Softmax(dim=1)(output.output_logits)\n","\n","input_id = tokenizer([datapoint[_]['card'] for _ in range(2)], padding=True, return_tensors='pt').input_ids.cuda()\n","print('Card: ' + datapoint[0]['card'])\n","#print(input_id)\n","outputs = model.lm.generate(input_id)\n","print(outputs.squeeze(0))\n","code = tokenizer.batch_decode(outputs)\n","print(code)\n","print(end)\n","#print('Ground-truth code: ' + datapoint[0]['code'])\n","print(type(tokenizer))\n","print(type(code))\n"],"id":"59e34677","execution_count":null,"outputs":[{"output_type":"stream","text":["Card: Moriok Rigger NAME_END 2 ATK_END 2 DEF_END {2}{B} COST_END NIL DUR_END Creature - Human Rogue Rigger TYPE_END Fifth Dawn PLAYER_CLS_END 54 RACE_END R RARITY_END Whenever an artifact is put into a graveyard from the battlefield , you may put a +1/+1 counter on Moriok Rigger .\n","\n","tensor([[     0,   2821,   3931,  48359,    925,    582,  48519,  42685,    263,\n","          12495, 205046,    785,   3792,  16077,    785,   3792,  10961,  48359,\n","            925,    582],\n","        [     0,   2821,   3931, 102372,    470,  18283, 127739,   1513,  42685,\n","            263,  12495, 205046,    785,   3792,  10961, 102372,    470,  18283,\n","         127739,   1513]], device='cuda:0')\n","['<pad> public class MoriokRigger extends CardImpl {§static {§public MoriokR', '<pad> public class AnthemOfRakdos extends CardImpl {§public AnthemOfRakdos']\n","<pad> public class AnthemManaCostImpl {§public AnthemManaCost(UUID\n","<class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>\n","<class 'list'>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fafbd5f5"},"source":["def autoregressive_generate(model, tokenizer, card_desc):\n","  '''\n","  Applies autoregressive generation on a card description to generate its corresponding code\n","\n","  Parameters\n","  ----------\n","  model: CardTranslationModel\n","      Model used for autoregressive generation\n","  tokenizer: transformers.models.tokenizer.PreTrainedTokenizer\n","      Tokenizer for encoding the card description\n","  card_desc: str or list[str] (batch_size length)\n","      card description\n","\n","  Returns\n","  ---------\n","  torch.Tensor containing the sequence of code ids generated from the card description\n","     shape: (batch_size, max_seq_len)\n","'''\n","  card_inputs = tokenizer(card_desc, padding=True, return_tensors='pt').input_ids.cuda()\n","  return model.lm.generate(card_inputs)\n","\n","def decode(tokenizer, code_ids):\n","  '''\n","  Translates code ids into tokens\n","\n","  Parameters\n","  ----------\n","  tokenizer: transformers.models.tokenizer.PreTrainedTokenizer\n","      Tokenizer for decoding\n","  code_ids: torch.Tensor\n","      shape: (batch_size, max_seq_len)\n","\n","  Returns\n","  ---------\n","  list containing the token seq for each id sequence in the batch\n","      shape (batch_size, max_seq_len)\n","  '''\n","  decode_output = tokenizer.batch_decode(code_ids)\n","  return decode_output\n"],"id":"fafbd5f5","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"095a050d"},"source":[""],"id":"095a050d","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d22592cc"},"source":[""],"id":"d22592cc","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f359ba1a"},"source":[""],"id":"f359ba1a","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8cc8400b"},"source":[""],"id":"8cc8400b","execution_count":null,"outputs":[]}]}